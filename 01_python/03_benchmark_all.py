#!/usr/bin/env python3
"""
03. å¤šåç«¯æ€§èƒ½æµ‹è¯•ï¼šTFLiteã€ONNXã€TensorRTã€NCNNå…¨é¢å¯¹æ¯”
ç¬¬ä¸‰æ­¥æ‰§è¡Œï¼šå¯¹æ¯”æ‰€æœ‰æ¨ç†åç«¯çš„æ€§èƒ½è¡¨ç°
è¿è¡Œå‰ç¡®ä¿: 01_infer_basic.py å’Œ 02_convert_model.py æˆåŠŸå®Œæˆ
"""

import argparse
import os
import time
import numpy as np
from PIL import Image

class InferenceBackend:
    """æ¨ç†åç«¯åŸºç±»"""
    
    def __init__(self, model_path):
        self.model_path = model_path
        self.input_name = None
        self.output_name = None
        self.input_shape = None
        self.output_shape = None
        self.input_dtype = None
        self.output_dtype = None
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        raise NotImplementedError
        
    def preprocess(self, image_path):
        """é¢„å¤„ç†å›¾åƒ"""
        raise NotImplementedError
        
    def inference(self, input_data):
        """æ‰§è¡Œæ¨ç†"""
        raise NotImplementedError
        
    def postprocess(self, output_data):
        """åå¤„ç†è¾“å‡º"""
        raise NotImplementedError
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        raise NotImplementedError

class TFLiteBackend(InferenceBackend):
    """TensorFlow Liteåç«¯"""
    
    def load_model(self):
        try:
            import tensorflow as tf
            
            print(f"åŠ è½½TFLiteæ¨¡å‹: {self.model_path}")
            self.interpreter = tf.lite.Interpreter(model_path=self.model_path)
            self.interpreter.allocate_tensors()
            
            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()
            
            self.input_name = self.input_details[0]['name']
            self.output_name = self.output_details[0]['name']
            self.input_shape = self.input_details[0]['shape']
            self.output_shape = self.output_details[0]['shape']
            self.input_dtype = self.input_details[0]['dtype']
            self.output_dtype = self.output_details[0]['dtype']
            
            print(f"âœ… TFLiteæ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   è¾“å…¥åç§°: {self.input_name}")
            print(f"   è¾“å…¥å½¢çŠ¶: {self.input_shape}")
            print(f"   è¾“å…¥æ•°æ®ç±»å‹: {self.input_dtype}")
            print(f"   è¾“å‡ºåç§°: {self.output_name}")
            print(f"   è¾“å‡ºå½¢çŠ¶: {self.output_shape}")
            print(f"   è¾“å‡ºæ•°æ®ç±»å‹: {self.output_dtype}")
            return True
            
        except Exception as e:
            print(f"âŒ TFLiteæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def preprocess(self, image_path):
        """TFLiteé¢„å¤„ç† (HWCæ ¼å¼) - MobileNetV2æ ‡å‡†åŒ–"""
        img = Image.open(image_path).resize((224, 224))
        img = np.array(img).astype(np.float32)
        # MobileNetV2æ ‡å‡†åŒ–: [-1, 1] èŒƒå›´
        img = (img - 127.5) / 127.5
        img = np.expand_dims(img, axis=0)
        return img
    
    def inference(self, input_data):
        """TFLiteæ¨ç†"""
        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
        self.interpreter.invoke()
        output = self.interpreter.get_tensor(self.output_details[0]['index'])
        return output
    
    def postprocess(self, output_data):
        """TFLiteåå¤„ç†ï¼šå‹ç¼©å¤šä½™ç»´åº¦"""
        return np.squeeze(output_data)
    
    def cleanup(self):
        """TFLiteæ¸…ç†èµ„æºï¼šé‡Šæ”¾è§£é‡Šå™¨"""
        if hasattr(self, 'interpreter') and self.interpreter is not None:
            # TensorFlow Liteè§£é‡Šå™¨ä¼šè‡ªåŠ¨ç®¡ç†å†…å­˜
            self.interpreter = None

class ONNXBackend(InferenceBackend):
    """ONNX Runtimeåç«¯"""
    
    def load_model(self):
        try:
            import onnxruntime as ort
            
            print(f"åŠ è½½ONNXæ¨¡å‹: {self.model_path}")
            print("ğŸ’» ä½¿ç”¨CPUä¼˜åŒ–æ‰§è¡Œ")
            
            # ä»…ä½¿ç”¨CPUæ‰§è¡Œæä¾›ç¨‹åº
            providers = ['CPUExecutionProvider']
            
            self.session = ort.InferenceSession(self.model_path, providers=providers)
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            self.input_name = self.session.get_inputs()[0].name
            self.output_name = self.session.get_outputs()[0].name
            self.input_shape = self.session.get_inputs()[0].shape
            self.output_shape = self.session.get_outputs()[0].shape
            self.input_dtype = self.session.get_inputs()[0].type
            self.output_dtype = self.session.get_outputs()[0].type
            
            print(f"âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   è¾“å…¥åç§°: {self.input_name}")
            print(f"   è¾“å…¥å½¢çŠ¶: {self.input_shape}")
            print(f"   è¾“å…¥æ•°æ®ç±»å‹: {self.input_dtype}")
            print(f"   è¾“å‡ºåç§°: {self.output_name}")
            print(f"   è¾“å‡ºå½¢çŠ¶: {self.output_shape}")
            print(f"   è¾“å‡ºæ•°æ®ç±»å‹: {self.output_dtype}")
            print(f"   æ‰§è¡Œæä¾›ç¨‹åº: {self.session.get_providers()}")
            return True
            
        except Exception as e:
            print(f"âŒ ONNXæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def preprocess(self, image_path):
        """ONNXé¢„å¤„ç† (CHWæ ¼å¼) - MobileNetV2æ ‡å‡†åŒ–"""
        img = Image.open(image_path).resize((224, 224))
        img = np.array(img).astype(np.float32)
        # MobileNetV2æ ‡å‡†åŒ–: [-1, 1] èŒƒå›´
        img = (img - 127.5) / 127.5
        img = np.transpose(img, (2, 0, 1))  # HWC -> CHW
        img = np.expand_dims(img, axis=0)
        return img
    
    def inference(self, input_data):
        """ONNXæ¨ç†"""
        outputs = self.session.run([self.output_name], {self.input_name: input_data})
        return outputs[0]
    
    def postprocess(self, output_data):
        """ONNXåå¤„ç†ï¼šå‹ç¼©å¤šä½™ç»´åº¦"""
        return np.squeeze(output_data)
    
    def cleanup(self):
        """ONNXæ¸…ç†èµ„æºï¼šé‡Šæ”¾ä¼šè¯"""
        if hasattr(self, 'session') and self.session is not None:
            # ONNXRuntimeä¼šè¯ä¼šè‡ªåŠ¨ç®¡ç†å†…å­˜
            self.session = None

class TensorRTBackend(InferenceBackend):
    """åŸç”ŸTensorRTåç«¯"""
    
    def load_model(self):
        try:
            import tensorrt as trt
            import pycuda.driver as cuda
            import pycuda.autoinit
            
            print(f"åŠ è½½TensorRTå¼•æ“: {self.model_path}")
            
            # åŠ è½½å¼•æ“
            TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
            with open(self.model_path, "rb") as f:
                runtime = trt.Runtime(TRT_LOGGER)
                self.engine = runtime.deserialize_cuda_engine(f.read())
            
            if self.engine is None:
                raise RuntimeError("å¼•æ“åŠ è½½å¤±è´¥")
            
            self.context = self.engine.create_execution_context()
            
            # è·å–ç»‘å®šä¿¡æ¯
            self.input_name = self.engine.get_tensor_name(0)
            self.output_name = self.engine.get_tensor_name(1)
            
            self.input_shape = self.engine.get_tensor_shape(self.input_name)
            self.output_shape = self.engine.get_tensor_shape(self.output_name)
            self.input_dtype = self.engine.get_tensor_dtype(self.input_name)
            self.output_dtype = self.engine.get_tensor_dtype(self.output_name)
            
            # åˆ†é…GPUå†…å­˜
            input_size = int(np.prod(self.input_shape) * np.dtype(np.float32).itemsize)
            output_size = int(np.prod(self.output_shape) * np.dtype(np.float32).itemsize)
            
            self.d_input = cuda.mem_alloc(input_size)
            self.d_output = cuda.mem_alloc(output_size)
            
            print(f"âœ… TensorRTå¼•æ“åŠ è½½æˆåŠŸ")
            print(f"   è¾“å…¥åç§°: {self.input_name}")
            print(f"   è¾“å…¥å½¢çŠ¶: {self.input_shape}")
            print(f"   è¾“å…¥æ•°æ®ç±»å‹: {self.input_dtype}")
            print(f"   è¾“å‡ºåç§°: {self.output_name}")
            print(f"   è¾“å‡ºå½¢çŠ¶: {self.output_shape}")
            print(f"   è¾“å‡ºæ•°æ®ç±»å‹: {self.output_dtype}")
            print(f"   GPUå†…å­˜: è¾“å…¥{input_size//1024}KB, è¾“å‡º{output_size//1024}KB")
            return True
            
        except Exception as e:
            print(f"âŒ TensorRTå¼•æ“åŠ è½½å¤±è´¥: {e}")
            return False
    
    def preprocess(self, image_path):
        """TensorRTé¢„å¤„ç† (CHWæ ¼å¼) - MobileNetV2æ ‡å‡†åŒ–"""
        img = Image.open(image_path).resize((224, 224))
        img = np.array(img).astype(np.float32)
        
        # MobileNetV2æ ‡å‡†åŒ–: [-1, 1] èŒƒå›´
        img = (img - 127.5) / 127.5
        
        img = np.transpose(img, (2, 0, 1))  # HWC -> CHW
        img = np.expand_dims(img, axis=0)
        return img
    
    def inference(self, input_data):
        """TensorRTæ¨ç†"""
        import pycuda.driver as cuda
        
        # æ‹·è´è¾“å…¥åˆ°GPU
        input_host = np.ascontiguousarray(input_data.ravel()).astype(np.float32)
        cuda.memcpy_htod(self.d_input, input_host)
        
        # æ‰§è¡Œæ¨ç† (ä½¿ç”¨ä¼ ç»Ÿçš„bindingæ–¹å¼)
        bindings = [int(self.d_input), int(self.d_output)]
        self.context.execute_v2(bindings)
        
        # æ‹·è´è¾“å‡ºå›CPU
        output_host = np.empty(self.output_shape, dtype=np.float32)
        cuda.memcpy_dtoh(output_host, self.d_output)
        
        return output_host
    
    def postprocess(self, output_data):
        """TensorRTåå¤„ç†ï¼šå‹ç¼©å¤šä½™ç»´åº¦"""
        return np.squeeze(output_data)
    
    def cleanup(self):
        """TensorRTæ¸…ç†èµ„æºï¼šé‡Šæ”¾GPUå†…å­˜"""
        try:
            if hasattr(self, 'd_input') and self.d_input is not None:
                self.d_input.free()
                self.d_input = None
            
            if hasattr(self, 'd_output') and self.d_output is not None:
                self.d_output.free()
                self.d_output = None
            
            if hasattr(self, 'context') and self.context is not None:
                # TensorRTä¸Šä¸‹æ–‡ä¼šè‡ªåŠ¨ç®¡ç†ï¼Œä½†æ˜¾å¼æ¸…ç†æ›´å®‰å…¨
                self.context = None
            
            if hasattr(self, 'engine') and self.engine is not None:
                # å¼•æ“ä¼šè‡ªåŠ¨ç®¡ç†ï¼Œä½†æ˜¾å¼æ¸…ç†æ›´å®‰å…¨
                self.engine = None
                
        except Exception as e:
            print(f"âš ï¸ TensorRTèµ„æºæ¸…ç†è­¦å‘Š: {e}")

class NCNNBackend(InferenceBackend):
    """NCNNåç«¯"""
    
    def load_model(self):
        try:
            import ncnn
            
            print(f"åŠ è½½NCNNæ¨¡å‹: {self.model_path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯.paramæ–‡ä»¶ï¼Œå¦‚æœæ˜¯ç›®å½•åˆ™å¯»æ‰¾.paramæ–‡ä»¶
            if os.path.isdir(self.model_path):
                param_files = [f for f in os.listdir(self.model_path) if f.endswith('.param')]
                if not param_files:
                    raise FileNotFoundError(f"ç›®å½•ä¸­æ‰¾ä¸åˆ°.paramæ–‡ä»¶: {self.model_path}")
                param_path = os.path.join(self.model_path, param_files[0])
                bin_path = param_path.replace('.param', '.bin')
            elif self.model_path.endswith('.param'):
                param_path = self.model_path
                bin_path = self.model_path.replace('.param', '.bin')
            else:
                # å‡è®¾ä¼ å…¥çš„æ˜¯å‰ç¼€
                param_path = self.model_path + '.param'
                bin_path = self.model_path + '.bin'
            
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
            if not os.path.exists(param_path):
                raise FileNotFoundError(f"å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {param_path}")
            if not os.path.exists(bin_path):
                raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {bin_path}")
            
            # åˆ›å»ºç½‘ç»œ
            self.net = ncnn.Net()
            
            # å¯ç”¨ä¼˜åŒ–é€‰é¡¹
            self.net.opt.use_vulkan_compute = False  # CPUæ¨¡å¼
            self.net.opt.use_fp16_packed = True      # å¯ç”¨FP16ä¼˜åŒ–
            self.net.opt.use_fp16_storage = True     # å¯ç”¨FP16å­˜å‚¨
            
            # åŠ è½½æ¨¡å‹æ–‡ä»¶
            ret_param = self.net.load_param(param_path)
            ret_model = self.net.load_model(bin_path)
            
            if ret_param != 0:
                raise RuntimeError(f"å‚æ•°æ–‡ä»¶åŠ è½½å¤±è´¥: {ret_param}")
            if ret_model != 0:
                raise RuntimeError(f"æ¨¡å‹æ–‡ä»¶åŠ è½½å¤±è´¥: {ret_model}")
            
            # âœ… é€šè¿‡APIè·å–ï¼šè¾“å…¥è¾“å‡ºåç§°
            input_names = self.net.input_names()
            output_names = self.net.output_names()
            
            self.input_name = input_names[0] if input_names else "input"
            self.output_name = output_names[0] if output_names else "output"
            
            # âœ… åˆç†å‡è®¾ï¼šå…¶å®ƒæ‰€æœ‰ä¿¡æ¯
            self.input_shape = (1, 3, 224, 224)  # MobileNetV2æ ‡å‡†è¾“å…¥
            self.input_dtype = "float32"          # NCNNå‡ ä¹æ€»æ˜¯float32
            self.output_shape = (1, 1001)        # MobileNetV2åˆ†ç±»è¾“å‡ºï¼ˆåŒ…å«èƒŒæ™¯ç±»ï¼‰
            self.output_dtype = "float32"         # NCNNå‡ ä¹æ€»æ˜¯float32
            
            print(f"âœ… NCNNæ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   è¾“å…¥åç§°: {self.input_name}")
            print(f"   è¾“å…¥å½¢çŠ¶: {self.input_shape}")
            print(f"   è¾“å…¥æ•°æ®ç±»å‹: {self.input_dtype}")
            print(f"   è¾“å‡ºåç§°: {self.output_name}")
            print(f"   è¾“å‡ºå½¢çŠ¶: {self.output_shape}")
            print(f"   è¾“å‡ºæ•°æ®ç±»å‹: {self.output_dtype}")
            print(f"   è·å–æ–¹å¼: APIåç§° + åˆç†å‡è®¾")
            print(f"   æ¨¡å‹æ–‡ä»¶: {os.path.basename(param_path)}, {os.path.basename(bin_path)}")
            return True
            
        except ImportError:
            print("âŒ NCNN Pythonç»‘å®šæœªå®‰è£…")
            print("   è¯·è¿è¡Œ: pip install ncnn")
            return False
        except Exception as e:
            print(f"âŒ NCNNæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def preprocess(self, image_path):
        """NCNNé¢„å¤„ç† (CHWæ ¼å¼) - MobileNetV2æ ‡å‡†åŒ–"""
        from PIL import Image
        img = Image.open(image_path).convert('RGB').resize((224, 224))
        img = np.array(img).astype(np.float32)
        
        # MobileNetV2æ ‡å‡†åŒ–: [-1, 1] èŒƒå›´
        img = (img - 127.5) / 127.5
        
        # NCNNçš„ncnn.Mat(array)éœ€è¦CHWæ ¼å¼
        # from_pixelsæ¥å£ä¼šå†…éƒ¨åšè¿™ä¸ªè½¬æ¢ï¼Œä½†ç›´æ¥ç”¨arrayéœ€è¦æ‰‹åŠ¨è½¬æ¢
        img = np.transpose(img, (2, 0, 1))
        return img
    
    def inference(self, input_data):
        """NCNNæ¨ç† - ç›´æ¥ä½¿ç”¨numpyé¢„å¤„ç†å¥½çš„CHWæ•°æ®"""
        try:
            import ncnn
            
            # input_dataæ˜¯é¢„å¤„ç†å¥½çš„float32 CHWæ•°æ® [-1, 1]
            # åˆ›å»ºncnn.Matï¼Œç¡®ä¿æ•°æ®æ˜¯è¿ç»­çš„
            input_data = np.ascontiguousarray(input_data)
            mat_in = ncnn.Mat(input_data)
            
            # åˆ›å»ºæå–å™¨
            ex = self.net.create_extractor()
            ex.input(self.input_name, mat_in)
            
            # æå–è¾“å‡ºï¼ˆä½¿ç”¨åŠ¨æ€è·å–çš„è¾“å‡ºåç§°ï¼‰
            mat_out = ncnn.Mat()
            ex.extract(self.output_name, mat_out)
            
            # è½¬æ¢ä¸º numpy æ•°ç»„
            output = np.array(mat_out)
            return output
            
        except Exception as e:
            raise RuntimeError(f"NCNNæ¨ç†å¤±è´¥: {e}")
    
    def postprocess(self, output_data):
        """NCNNåå¤„ç†ï¼šå‹ç¼©å¤šä½™ç»´åº¦"""
        return np.squeeze(output_data)
    
    def cleanup(self):
        """NCNNæ¸…ç†èµ„æºï¼šé‡Šæ”¾ç½‘ç»œ"""
        if hasattr(self, 'net') and self.net is not None:
            # NCNNç½‘ç»œä¼šè‡ªåŠ¨ç®¡ç†å†…å­˜
            self.net = None

def benchmark_model(backend, image_path, test_runs=20):
    """æ€§èƒ½æµ‹è¯•"""
    print(f"\n=== æ€§èƒ½æµ‹è¯• ===")
    print(f"æµ‹è¯•æ¬¡æ•°: {test_runs}")
    
    # åŠ è½½æ¨¡å‹
    if not backend.load_model():
        return None, None
    
    try:
        # é¢„å¤„ç†å›¾åƒ
        print("æ­£åœ¨é¢„å¤„ç†å›¾åƒ...")
        preprocess_start = time.time()
        input_data = backend.preprocess(image_path)
        preprocess_time = time.time() - preprocess_start
        
        # æ€§èƒ½æµ‹è¯•
        print("æ­£åœ¨æ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
        inference_times = []
        postprocess_time = None
        predictions = None
        
        for i in range(test_runs):
            try:
                start = time.time()
                output = backend.inference(input_data)
                inference_time = time.time() - start
                inference_times.append(inference_time)
                
                if i == 0:
                    postprocess_start = time.time()
                    predictions = backend.postprocess(output)
                    postprocess_time = time.time() - postprocess_start
                    print("é¦–æ¬¡æ¨ç†å®Œæˆ")
                    
            except Exception as e:
                print(f"æ¨ç†å¤±è´¥ (ç¬¬{i+1}æ¬¡): {e}")
                return None, None
                
    finally:
        # æ¸…ç†èµ„æº
        backend.cleanup()
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    avg_time = np.mean(inference_times)
    min_time = np.min(inference_times)
    max_time = np.max(inference_times)
    std_time = np.std(inference_times)
    
    print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print(f"   é¢„å¤„ç†è€—æ—¶: {preprocess_time:.4f}ç§’")
    print(f"   å¹³å‡æ¨ç†è€—æ—¶: {avg_time:.4f}ç§’ ({1/avg_time:.1f} FPS)")
    print(f"   æœ€å¿«æ¨ç†è€—æ—¶: {min_time:.4f}ç§’ ({1/min_time:.1f} FPS)")
    print(f"   æœ€æ…¢æ¨ç†è€—æ—¶: {max_time:.4f}ç§’ ({1/max_time:.1f} FPS)")
    print(f"   æ ‡å‡†å·®: {std_time:.4f}ç§’")
    if postprocess_time is not None:
        print(f"   åå¤„ç†è€—æ—¶: {postprocess_time:.4f}ç§’")
    
    return avg_time, predictions

def show_predictions(predictions, labels_path=None, top_k=5):
    """æ˜¾ç¤ºé¢„æµ‹ç»“æœ"""
    if predictions is None:
        return
    
    # åŠ è½½æ ‡ç­¾
    try:
        if labels_path and os.path.exists(labels_path):
            with open(labels_path, 'r') as f:
                labels = f.read().splitlines()
        else:
            labels = [f"class_{i}" for i in range(len(predictions))]
    except:
        labels = [f"class_{i}" for i in range(len(predictions))]
    
    # è·å–Top-K
    top_indices = predictions.argsort()[-top_k:][::-1]
    
    for i, idx in enumerate(top_indices):
        class_name = f"class_{idx}"
        real_label = labels[idx] if idx < len(labels) else f"unknown_{idx}"
        confidence = predictions[idx]
        print(f"   {i+1}. {class_name} ({real_label}): {confidence:.4f} ({confidence*100:.1f}%)")

def main():
    parser = argparse.ArgumentParser(description="æ¨ç†æ€§èƒ½æµ‹è¯•å·¥å…·")
    parser.add_argument("--tflite", help="TFLiteæ¨¡å‹è·¯å¾„")
    parser.add_argument("--onnx", help="ONNXæ¨¡å‹è·¯å¾„")
    parser.add_argument("--tensorrt-fp32", help="TensorRT FP32å¼•æ“è·¯å¾„")
    parser.add_argument("--tensorrt-fp16", help="TensorRT FP16å¼•æ“è·¯å¾„") 
    parser.add_argument("--tensorrt-int8", help="TensorRT INT8å¼•æ“è·¯å¾„")
    parser.add_argument("--ncnn", help="NCNNæ¨¡å‹è·¯å¾„ (.paramæ–‡ä»¶æˆ–ç›®å½•)")
    parser.add_argument("--ncnn-int8", help="NCNN INT8é‡åŒ–æ¨¡å‹è·¯å¾„ (.paramæ–‡ä»¶æˆ–ç›®å½•)")
    parser.add_argument("--image", "-i", required=True, help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--labels", help="æ ‡ç­¾æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--runs", type=int, default=20, help="æµ‹è¯•æ¬¡æ•° (é»˜è®¤: 20)")
    parser.add_argument("--top-k", type=int, default=5, help="æ˜¾ç¤ºTop-Kç»“æœ (é»˜è®¤: 5)")
    parser.add_argument("--compare", action="store_true", help="å¯¹æ¯”æ‰€æœ‰å¯ç”¨æ¨¡å‹")
    parser.add_argument("--ncnn-accuracy-warning", action="store_true", help="æ˜¾ç¤ºNCNNç²¾åº¦è­¦å‘Š")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.image):
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {args.image}")
        return 1
    
    print("=== æ¨ç†æ€§èƒ½æµ‹è¯•å·¥å…· ===")
    print(f"è¾“å…¥å›¾åƒ: {args.image}")
    
    results = {}
    all_predictions = {}
    
    # æµ‹è¯•TFLite
    if args.tflite and os.path.exists(args.tflite):
        print(f"\nğŸ”µ === TensorFlow Lite æµ‹è¯• ===")
        backend = TFLiteBackend(args.tflite)
        avg_time, predictions = benchmark_model(backend, args.image, args.runs)
        if avg_time is not None:
            results['TensorFlow Lite'] = avg_time
            all_predictions['TensorFlow Lite'] = predictions
    
    # æµ‹è¯•ONNX (CPUä¼˜åŒ–)
    if args.onnx and os.path.exists(args.onnx):
        print(f"\nğŸŸ  === ONNX Runtime æµ‹è¯• ===")
        backend = ONNXBackend(args.onnx)
        avg_time, predictions = benchmark_model(backend, args.image, args.runs)
        if avg_time is not None:
            results['ONNX (CPU)'] = avg_time
            all_predictions['ONNX (CPU)'] = predictions
    
    # æµ‹è¯•TensorRTå„ç²¾åº¦æ¨¡å¼
    tensorrt_configs = [
        (args.tensorrt_fp32, "TensorRT (FP32)", "ğŸ”´"),
        (args.tensorrt_fp16, "TensorRT (FP16)", "ğŸŸ "), 
        (args.tensorrt_int8, "TensorRT (INT8)", "ğŸŸ¡")
    ]
    
    for trt_path, name, emoji in tensorrt_configs:
        if trt_path and os.path.exists(trt_path):
            print(f"\n{emoji} === {name} æµ‹è¯• ===")
            backend = TensorRTBackend(trt_path)
            avg_time, predictions = benchmark_model(backend, args.image, args.runs)
            if avg_time is not None:
                results[name] = avg_time
                all_predictions[name] = predictions
    
    # æµ‹è¯•NCNNå„æ¨¡å¼
    ncnn_configs = [
        (args.ncnn, "NCNN", "ğŸŸ¢"),
        (args.ncnn_int8, "NCNN (INT8)", "ğŸŸ«")
    ]
    
    for ncnn_path, name, emoji in ncnn_configs:
        if ncnn_path and os.path.exists(ncnn_path):
            print(f"\n{emoji} === {name} æµ‹è¯• ===")
            backend = NCNNBackend(ncnn_path)
            avg_time, predictions = benchmark_model(backend, args.image, args.runs)
            if avg_time is not None:
                results[name] = avg_time
                all_predictions[name] = predictions
    
    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    if results:
        print(f"\nğŸ† === æ€§èƒ½å¯¹æ¯”ç»“æœ ===")
        fastest_time = min(results.values())
        
        for backend, avg_time in sorted(results.items(), key=lambda x: x[1]):
            fps = 1 / avg_time
            if avg_time == fastest_time:
                print(f"{backend:20}: {avg_time:.4f}ç§’ ({fps:6.1f} FPS) ğŸ¥‡ [æœ€å¿«]")
            else:
                speedup = avg_time / fastest_time
                print(f"{backend:20}: {avg_time:.4f}ç§’ ({fps:6.1f} FPS) [{speedup:.1f}x slower]")
        
        # æ˜¾ç¤ºæ¯ä¸ªåç«¯çš„é¢„æµ‹ç»“æœ
        print(f"\nğŸ” === å„åç«¯é¢„æµ‹ç»“æœå¯¹æ¯” ===")
        for backend_name, predictions in all_predictions.items():
            print(f"\nã€{backend_name}ã€‘é¢„æµ‹ç»“æœ:")
            if 'NCNN' in backend_name and args.ncnn_accuracy_warning:
                print("âš ï¸  NCNNé¢„æµ‹ç²¾åº¦å¯èƒ½ä¸å…¶ä»–æ¡†æ¶å­˜åœ¨å·®å¼‚ï¼Œå»ºè®®ä»…ç”¨äºæ€§èƒ½æµ‹è¯•")
            show_predictions(predictions, args.labels, args.top_k)
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())