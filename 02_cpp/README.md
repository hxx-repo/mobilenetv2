# MobileNetV2 C++ æ¨ç†å¼•æ“

é«˜æ€§èƒ½ MobileNetV2 æ·±åº¦å­¦ä¹ æ¨¡å‹ C++ æ¨ç†å®ç°ï¼Œ**å®Œå…¨å¯¹é½Pythonç‰ˆæœ¬æ¶æ„è®¾è®¡**ã€‚

## ğŸ“Š æ€§èƒ½ç»“æœ

åœ¨ RTX 3060 ä¸Šçš„ MobileNetV2 æ¨ç†æ€§èƒ½å¯¹æ¯”ï¼ˆå®Œå…¨å¯¹é½Pythoné¢„å¤„ç†ï¼‰ï¼š

```
ğŸ† === C++ æ€§èƒ½å¯¹æ¯”ç»“æœ ===
TensorRT (FP16)     : 0.0010ç§’ ( 994.7 FPS) ğŸ¥‡ [æœ€å¿«GPU] - 9MB
TensorRT (FP32)     : 0.0013ç§’ ( 749.6 FPS) [1.3x slower] ğŸ¥ˆ [æœ€å¿«GPU] - 16MB  
TensorRT (INT8)     : 0.0012ç§’ ( 834.5 FPS) [1.2x slower] ğŸ¥‰ [æœ€å¿«GPU] - 5MB
NCNN FP32           : 0.0029ç§’ ( 341.3 FPS) [2.9x slower] [æœ€å¿«CPU] - 14MB
NCNN INT8           : 0.0041ç§’ ( 242.2 FPS) [4.1x slower] [ç§»åŠ¨ä¼˜åŒ–] - 3.5MB
MNN FP32           : 0.0035ç§’ ( 285.2 FPS) [3.5x slower] [ç«¯ä¾§éƒ¨ç½²] - 14MB
MNN INT8           : 0.0043ç§’ ( 234.0 FPS) [4.3x slower] [ç«¯ä¾§éƒ¨ç½²] - 3.6MB
ONNX Runtime        : 0.0130ç§’ ( 77.1 FPS) [12.9x slower] [è·¨å¹³å°] - 14MB
TensorFlow Lite     : 0.0209ç§’ ( 47.8 FPS) [20.9x slower] [åŸºå‡†] - 14MB
```

**ç²¾åº¦éªŒè¯**ï¼šæ‰€æœ‰åç«¯ä¸Pythonç‰ˆæœ¬æ•°å€¼å®Œå…¨ä¸€è‡´

| åç«¯ | Top-1é¢„æµ‹ | ç½®ä¿¡åº¦ | ç²¾åº¦çŠ¶æ€ |
| ---- | --------- | ------ | -------- |
| **TensorFlow Lite** | goldfish | 99.73% | âœ… ä¸Pythonå®Œå…¨åŒ¹é… |
| **ONNX Runtime** | goldfish | 99.73% | âœ… ä¸Pythonå®Œå…¨åŒ¹é… |
| **TensorRT (FP32)** | goldfish | 99.73% | âœ… ä¸Pythonå®Œå…¨åŒ¹é… |
| **TensorRT (FP16)** | goldfish | 99.76% | âœ… ä¸Pythonå®Œå…¨åŒ¹é… |
| **NCNN FP32** | goldfish | 99.73% | âœ… ä¸Pythonå®Œå…¨åŒ¹é… |
| **TensorRT (INT8)** | goldfish | 99.22% | âœ… é‡åŒ–è½»å¾®ç²¾åº¦æŸå¤±0.5% |
| **NCNN INT8** | goldfish | 99.38% | âœ… é‡åŒ–ç²¾åº¦è½»å¾®æŸå¤±0.35% |
| **MNN** | goldfish | 99.80% | âœ… ä¸Pythonå®Œå…¨åŒ¹é… |
| **MNN (INT8)** | goldfish | 99.50% | âœ… é‡åŒ–ç²¾åº¦è½»å¾®æŸå¤±0.3% |

## ğŸ› ï¸ ç¯å¢ƒé…ç½®

### ç³»ç»Ÿè¦æ±‚
- Ubuntu 18.04+ / WSL2
- CMake 3.16+
- C++17 ç¼–è¯‘å™¨ (GCC 7+)
- OpenCV 4.x
- NVIDIA GPU (RTX 20/30/40ç³»åˆ—) - ä»… TensorRT éœ€è¦
- CUDA 12.x + cuDNN - ä»… TensorRT éœ€è¦

### ä¾èµ–è¯´æ˜

```
C++åç«¯ä¾èµ–é¡ºåºï¼š
1. OpenCV - å›¾åƒé¢„å¤„ç†ï¼Œæ‰€æœ‰åç«¯å¿…éœ€
2. TensorFlow Lite - CPUè·¨å¹³å°æ¨ç†
3. ONNX Runtime - CPUä¼˜åŒ–æ¨ç†
4. TensorRT - GPUåŠ é€Ÿæ¨ç† (éœ€è¦CUDAç¯å¢ƒ)
5. NCNN - ç§»åŠ¨ç«¯ä¼˜åŒ–æ¨ç†
6. MNN - ç«¯ä¾§æ¨ç† / é‡åŒ–å·¥å…·é“¾
```

### å®‰è£…æ­¥éª¤

**CMake**

```bash
# ä¸‹è½½CMakeé¢„ç¼–è¯‘åŒ…
cd ~/work/depend_config/cmake
wget https://github.com/Kitware/CMake/releases/download/v3.28.1/cmake-3.28.1-linux-x86_64.tar.gz
tar -xzf cmake-3.28.1-linux-x86_64.tar.gz
export PATH="/home/xinxin/work/depend_config/cmake/cmake-3.28.1-linux-x86_64/bin:$PATH"

# å®‰è£…OpenCV (æ‰€æœ‰åç«¯å¿…éœ€)
# å‚è€ƒé…ç½®è·¯å¾„: /home/xinxin/work/depend_config/opencv/opencv_install
export OPENCV_ROOT="/home/xinxin/work/depend_config/opencv/opencv_install"
export LD_LIBRARY_PATH=$OPENCV_ROOT/lib:$LD_LIBRARY_PATH
```

**OpenCV**

```bash
# ä¸‹è½½OpenCVæºç 
cd ~/work/depend_config/opencv
wget https://github.com/opencv/opencv/archive/refs/tags/4.8.1.tar.gz -O opencv-4.8.1.tar.gz
tar -xzf opencv-4.8.1.tar.gz

# ç¼–è¯‘å®‰è£…
cd opencv-4.8.1 && mkdir build && cd build
cmake -D CMAKE_BUILD_TYPE=Release \
      -D CMAKE_INSTALL_PREFIX=../../opencv_install \
      -D BUILD_SHARED_LIBS=ON \
      -D BUILD_EXAMPLES=OFF \
      -D BUILD_TESTS=OFF \
      -D BUILD_opencv_apps=OFF \
      -D BUILD_opencv_python2=OFF \
      -D BUILD_opencv_python3=OFF \
      -D BUILD_PERF_TESTS=OFF \
      -D BUILD_DOCS=OFF \
      -D WITH_IPP=OFF ..
make -j$(nproc)
make install

# éªŒè¯å®‰è£…
ls /home/xinxin/work/depend_config/opencv/opencv_install/lib/
```

**TensorFlow Lite (CPUåç«¯)**

```bash
# ä¸‹è½½TensorFlowæºç 
cd ~/work/depend_config/tensorflow_lite
wget https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.16.1.tar.gz -O tensorflow-2.16.1.tar.gz
tar -xzf tensorflow-2.16.1.tar.gz

# ç”±äºç½‘ç»œé™åˆ¶ï¼Œéœ€è¦æ‰‹åŠ¨ä¸‹è½½ä»¥ä¸‹TensorFlow Liteä¾èµ–åŒ…
# abseil-cppã€eigenã€fft2d (OouraFFT)ã€neon2sseã€ml_dtypesã€cpuinfoã€farmhashã€flatbuffersã€gemmlowpã€ruy
```

```bash
# ä¸‹è½½abseil-cpp
cd ~/work/depend_config/tensorflow_lite
wget https://github.com/abseil/abseil-cpp/archive/fb3621f4f897824c0dbe0615fa94543df6192f30.tar.gz
tar -xzf fb3621f4f897824c0dbe0615fa94543df6192f30.tar.gz
```

```bash
# ä¸‹è½½eigen
cd ~/work/depend_config/tensorflow_lite
wget https://gitlab.com/libeigen/eigen/-/archive/aa6964bf3a34fd607837dd8123bc42465185c4f8/eigen-aa6964bf3a34fd607837dd8123bc42465185c4f8.tar.gz
tar -xzf eigen-aa6964bf3a34fd607837dd8123bc42465185c4f8.tar.gz
```

```bash
# ä¸‹è½½fft2d (OouraFFT)
cd ~/work/depend_config/tensorflow_lite
wget https://storage.googleapis.com/mirror.tensorflow.org/github.com/petewarden/OouraFFT/archive/v1.0.tar.gz -O OouraFFT-1.0.tar.gz
tar -xzf OouraFFT-1.0.tar.gz
```

```bash
# ä¸‹è½½neon2sse
cd ~/work/depend_config/tensorflow_lite
wget https://github.com/intel/ARM_NEON_2_x86_SSE/archive/a15b489e1222b2087007546b4912e21293ea86ff.tar.gz
tar -xzf a15b489e1222b2087007546b4912e21293ea86ff.tar.gz
```

```bash
# ä¸‹è½½ml_dtypes
cd ~/work/depend_config/tensorflow_lite
wget https://github.com/jax-ml/ml_dtypes/archive/780b6d0ee01ffbfac45f7ec5418bc08f2b166483.tar.gz
tar -xzf 780b6d0ee01ffbfac45f7ec5418bc08f2b166483.tar.gz
```

```bash
# ä¸‹è½½cpuinfo
cd ~/work/depend_config/tensorflow_lite
wget https://github.com/pytorch/cpuinfo/archive/ef634603954d88d2643d5809011288b890ac126e.tar.gz
tar -xzf ef634603954d88d2643d5809011288b890ac126e.tar.gz
```

```bash
# ä¸‹è½½farmhash
cd ~/work/depend_config/tensorflow_lite
wget https://github.com/google/farmhash/archive/0d859a811870d10f53a594927d0d0b97573ad06d.tar.gz
tar -xzf 0d859a811870d10f53a594927d0d0b97573ad06d.tar.gz
```

```bash
# ä¸‹è½½flatbuffers
cd ~/work/depend_config/tensorflow_lite
wget https://github.com/google/flatbuffers/archive/refs/tags/v23.5.26.tar.gz
tar -xzf v23.5.26.tar.gz
```

```bash
# ä¸‹è½½gemmlowp
cd ~/work/depend_config/tensorflow_lite
wget https://github.com/google/gemmlowp/archive/16e8662c34917be0065110bfcd9cc27d30f52fdf.tar.gz
tar -xzf 16e8662c34917be0065110bfcd9cc27d30f52fdf.tar.gz
```

```bash
# ä¸‹è½½ruy
cd ~/work/depend_config/tensorflow_lite
wget https://github.com/google/ruy/archive/3286a34cc8de6149ac6844107dfdffac91531e72.tar.gz
tar -xzf 3286a34cc8de6149ac6844107dfdffac91531e72.tar.gz
```

```bash
# éªŒè¯ä¾èµ–åŒ…
cd ~/work/depend_config/tensorflow_lite
ls -la | grep -E "(abseil-cpp|eigen|OouraFFT|ARM_NEON|ml_dtypes|cpuinfo|farmhash|flatbuffers|gemmlowp|ruy)"

# åº”è¯¥çœ‹åˆ°ä»¥ä¸‹ç›®å½•:
# abseil-cpp-fb3621f4f897824c0dbe0615fa94543df6192f30/
# eigen-aa6964bf3a34fd607837dd8123bc42465185c4f8/
# OouraFFT-1.0/
# ARM_NEON_2_x86_SSE-a15b489e1222b2087007546b4912e21293ea86ff/
# ml_dtypes-780b6d0ee01ffbfac45f7ec5418bc08f2b166483/
# cpuinfo-ef634603954d88d2643d5809011288b890ac126e/
# farmhash-0d859a811870d10f53a594927d0d0b97573ad06d/
# flatbuffers-23.5.26/
# gemmlowp-16e8662c34917be0065110bfcd9cc27d30f52fdf/
# ruy-3286a34cc8de6149ac6844107dfdffac91531e72/
```

```bash
# ä¿®æ”¹abseil-cppé…ç½®
sed -i 's|^.*git.*abseil-cpp.*|# Use local abseil-cpp instead of downloading\nset(abseil-cpp_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/abseil-cpp-fb3621f4f897824c0dbe0615fa94543df6192f30")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/abseil-cpp.cmake

# ä¿®æ”¹eigené…ç½®  
sed -i 's|^.*git.*eigen.*|# Use local eigen instead of downloading\nset(eigen_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/eigen-aa6964bf3a34fd607837dd8123bc42465185c4f8")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/eigen.cmake

# ä¿®æ”¹fft2dé…ç½®
sed -i 's|^.*git.*fft2d.*|# Use local fft2d instead of downloading\nset(fft2d_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/OouraFFT-1.0")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/fft2d.cmake

# ä¿®æ”¹neon2sseé…ç½®
sed -i 's|^.*git.*neon2sse.*|# Use local neon2sse instead of downloading\nset(neon2sse_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/ARM_NEON_2_x86_SSE-a15b489e1222b2087007546b4912e21293ea86ff")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/neon2sse.cmake

# ä¿®æ”¹ml_dtypesé…ç½®
sed -i 's|^.*git.*ml_dtypes.*|# Use local ml_dtypes instead of downloading\nset(ml_dtypes_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/ml_dtypes-780b6d0ee01ffbfac45f7ec5418bc08f2b166483")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/ml_dtypes.cmake

# ä¿®æ”¹cpuinfoé…ç½®
sed -i 's|^.*git.*cpuinfo.*|# Use local cpuinfo instead of downloading\nset(cpuinfo_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/cpuinfo-ef634603954d88d2643d5809011288b890ac126e")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/cpuinfo.cmake

# ä¿®æ”¹farmhashé…ç½®
sed -i 's|^.*git.*farmhash.*|# Use local farmhash instead of downloading\nset(farmhash_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/farmhash-0d859a811870d10f53a594927d0d0b97573ad06d")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/farmhash.cmake

# ä¿®æ”¹flatbuffersé…ç½®
sed -i 's|^.*git.*flatbuffers.*|# Use local flatbuffers instead of downloading\nset(flatbuffers_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/flatbuffers-23.5.26")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/flatbuffers.cmake

# ä¿®æ”¹gemmlowpé…ç½®
sed -i 's|^.*git.*gemmlowp.*|# Use local gemmlowp instead of downloading\nset(gemmlowp_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/gemmlowp-16e8662c34917be0065110bfcd9cc27d30f52fdf")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/gemmlowp.cmake

# ä¿®æ”¹ruyé…ç½®
sed -i 's|^.*git.*ruy.*|# Use local ruy instead of downloading\nset(ruy_SOURCE_DIR "/home/xinxin/work/depend_config/tensorflow_lite/ruy-3286a34cc8de6149ac6844107dfdffac91531e72")|' \
/home/xinxin/work/depend_config/tensorflow_lite/tensorflow-2.16.1/tensorflow/lite/tools/cmake/modules/ruy.cmake
```

```bash
# ç¼–è¯‘TensorFlow Lite
cd ~/work/depend_config/tensorflow_lite
mkdir tflite_build && cd tflite_build

# CMakeé…ç½®
cmake ../tensorflow-2.16.1/tensorflow/lite \
    -DTFLITE_ENABLE_XNNPACK=OFF \
    -DTFLITE_ENABLE_GPU=OFF \
    -DBUILD_SHARED_LIBS=ON

# ç¼–è¯‘ (å¹¶è¡Œç¼–è¯‘ï¼Œä½†éœ€è¦è¾ƒé•¿æ—¶é—´)
make -j$(nproc)

# éªŒè¯ç¼–è¯‘ç»“æœ
ls -la libtensorflow-lite.so
# åº”è¯¥çœ‹åˆ°: -rwxr-xr-x 1 xinxin xinxin 5807944 Sep  5 22:35 libtensorflow-lite.so
```

```bash
# é…ç½®TensorFlow Lite
export TFLITE_BUILD="/home/xinxin/work/depend_config/tensorflow_lite/tflite_build"
export LD_LIBRARY_PATH=$TFLITE_BUILD:$LD_LIBRARY_PATH
```

**ONNX Runtime (CPUåç«¯)**

```bash
# ä¸‹è½½ONNX Runtimeé¢„ç¼–è¯‘åŒ…
cd ~/work/depend_config/onnxruntime
wget https://github.com/microsoft/onnxruntime/releases/download/v1.19.2/onnxruntime-linux-x64-1.19.2.tgz
tar -xzf onnxruntime-linux-x64-1.19.2.tgz

# éªŒè¯è§£å‹ç»“æœ
ls -la onnxruntime-linux-x64-1.19.2/
# åº”è¯¥çœ‹åˆ°:
# include/          # C++ å¤´æ–‡ä»¶
# lib/              # é¢„ç¼–è¯‘åº“æ–‡ä»¶ 
# â”œâ”€â”€ libonnxruntime.so
# â”œâ”€â”€ libonnxruntime.so.1
# â””â”€â”€ libonnxruntime.so.1.19.2
```

```bash
# é…ç½®ONNX Runtime
export ONNXRUNTIME_ROOT="/home/xinxin/work/depend_config/onnxruntime/onnxruntime-linux-x64-1.19.2"
export LD_LIBRARY_PATH=$ONNXRUNTIME_ROOT/lib:$LD_LIBRARY_PATH
```

**TensorRT (GPUåç«¯)**

```bash
# **å‰ç½®æ¡ä»¶æ£€æŸ¥**:

# é…ç½®CUDA (TensorRTéœ€è¦)
export LD_LIBRARY_PATH=/usr/local/cuda-12.4/targets/x86_64-linux/lib:$LD_LIBRARY_PATH

# æ£€æŸ¥CUDAç‰ˆæœ¬ (éœ€è¦CUDA 11.0+)
nvidia-smi
nvcc --version
```

```bash
# ä¸‹è½½TensorRT 8.6.1.6
cd ~/work/depend_config/tensorrt
wget -c -q --show-progress "https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/secure/8.6.1/tars/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz"
tar -xzf TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz

# éªŒè¯è§£å‹ç»“æœ
ls -la TensorRT-8.6.1.6/
# åº”è¯¥çœ‹åˆ°:
# include/          # TensorRT C++ å¤´æ–‡ä»¶ (NvInfer.h, NvInferRuntime.hç­‰)
# lib/              # TensorRTåº“æ–‡ä»¶ (libnvinfer.soç­‰)
# bin/              # TensorRTå·¥å…· (trtexecç­‰)
```

```bash
# é…ç½®TensorRT
export TENSORRT_ROOT="/home/xinxin/work/depend_config/tensorrt/TensorRT-8.6.1.6"
export LD_LIBRARY_PATH=$TENSORRT_ROOT/lib:$LD_LIBRARY_PATH
export PATH=$TENSORRT_ROOT/bin:$PATH
```

**NCNN (ç§»åŠ¨ç«¯åç«¯)**

```bash
# ä¸‹è½½NCNNé¢„ç¼–è¯‘åŒ…
cd ~/work/depend_config/ncnn
wget https://github.com/Tencent/ncnn/releases/download/20231027/ncnn-20231027-ubuntu-2004-shared.zip
unzip ncnn-20231027-ubuntu-2004-shared.zip

# éªŒè¯è§£å‹ç»“æœ
ls -la ncnn-20231027-ubuntu-2004-shared/
# åº”è¯¥çœ‹åˆ°:
# include/ncnn/       # NCNN C++ å¤´æ–‡ä»¶ (net.h, mat.hç­‰)
# lib/                # NCNNåº“æ–‡ä»¶ (libncnn.soç­‰)
# bin/                # NCNNå·¥å…·

# ç”±äºç½‘ç»œé™åˆ¶ï¼Œéœ€è¦æ‰‹åŠ¨ä¸‹è½½ä»¥ä¸‹NCNNä¾èµ–åŒ…
# Vulkanã€protobuf
```

```bash
# ä¸‹è½½Vulkan (NCNNç¼–è¯‘ä¾èµ–)
cd ~/work/depend_config/ncnn
wget https://github.com/KhronosGroup/Vulkan-Headers/archive/refs/tags/v1.3.216.tar.gz
tar -xzf v1.3.216.tar.gz

export VULKAN_HEADERS_ROOT="/home/xinxin/work/depend_config/ncnn/Vulkan-Headers-1.3.216"
```

```bash
# ä¸‹è½½protobuf (NCNNè¿è¡Œæ—¶ä¾èµ–)
cd ~/work/depend_config/ncnn
wget http://archive.ubuntu.com/ubuntu/pool/main/p/protobuf/libprotobuf17_3.6.1.3-2ubuntu5_amd64.deb
ar x libprotobuf17_3.6.1.3-2ubuntu5_amd64.deb
tar -xf data.tar.xz

export PROTOBUF_ROOT="/home/xinxin/work/depend_config/ncnn/usr"
export LD_LIBRARY_PATH=$PROTOBUF_ROOT/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH
```

```bash
# é…ç½®NCNN
export NCNN_ROOT="/home/xinxin/work/depend_config/ncnn/ncnn-20231027-ubuntu-2004-shared"
export LD_LIBRARY_PATH=$NCNN_ROOT/lib:$LD_LIBRARY_PATH
```

**MNN (ç«¯ä¾§åç«¯)**

```bash
# è·å–å¹¶ç¼–è¯‘ MNN 3.2.5 (ä»…éœ€ lib)
cd ~/work/depend_config/mnn
git clone https://github.com/alibaba/MNN.git
cd MNN
git checkout tags/3.2.5
mkdir -p build && cd build
cmake .. -DMNN_BUILD_CONVERTER=false -DMNN_BUILD_QUANTOOLS=OFF
make -j$(nproc)

# æ•´ç†è¿è¡Œåº“
mkdir -p ~/work/depend_config/mnn/lib
ln -sf ~/work/depend_config/mnn/MNN/build/libMNN.so ~/work/depend_config/mnn/lib/libMNN.so

# é…ç½®ç¯å¢ƒå˜é‡ï¼ˆæ— éœ€è½¬æ¢/é‡åŒ–å·¥å…·ï¼‰
export MNN_ROOT="/home/xinxin/work/depend_config/mnn/MNN"
export LD_LIBRARY_PATH=~/work/depend_config/mnn/lib:$LD_LIBRARY_PATH
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

æŒ‰ç…§ç¼–å·é¡ºåºæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

### ç¼–è¯‘é¡¹ç›®

éœ€è¦å…ˆè®¾ç½®CMakeè·¯å¾„ï¼š

```bash
# CMakeè·¯å¾„ï¼ˆç¼–è¯‘æ—¶å¿…éœ€ï¼‰
export PATH="/home/xinxin/work/depend_config/cmake/cmake-3.28.1-linux-x86_64/bin:$PATH"

cd 02_cpp

# æ­¥éª¤1: æ„å»ºå¹¶å®‰è£…åŠ¨æ€åº“
./build_so.sh

# æ­¥éª¤2: æ„å»ºå¹¶å®‰è£…å¯æ‰§è¡Œæ–‡ä»¶  
./build_exe.sh
```

### è¿è¡Œæµ‹è¯•

éœ€è¦å…ˆè®¾ç½®è¿è¡Œæ—¶åº“è·¯å¾„ï¼š
```bash
# è®¾ç½®è¿è¡Œæ—¶åº“è·¯å¾„ï¼ˆå®‰è£…ç›®å½•é‡Œçš„ libmobilenet_inference.so æ²¡æœ‰è®°å½•ä¾èµ–åº“çš„è·¯å¾„ï¼ˆä»…é“¾æ¥åˆ°ç¼–è¯‘æ—¶çš„ç»å¯¹ä½ç½®ï¼‰ï¼Œè¿™ä¸ªæ­¥éª¤åº”è¯¥å¾—æ”¾åœ¨ æ­¥éª¤2: æ„å»ºå¹¶å®‰è£…å¯æ‰§è¡Œæ–‡ä»¶ ä¹‹å‰ï¼Œä¸ç„¶ä¸Šé¢æ­¥éª¤2ä¼šæŠ¥é”™ï¼‰
export LD_LIBRARY_PATH="\
/home/xinxin/work/depend_config/opencv/opencv_install/lib:\
/home/xinxin/work/depend_config/tensorflow_lite/tflite_build:\
/home/xinxin/work/depend_config/onnxruntime/onnxruntime-linux-x64-1.19.2/lib:\
/home/xinxin/work/depend_config/tensorrt/TensorRT-8.6.1.6/lib:\
/home/xinxin/work/depend_config/ncnn/ncnn-20231027-ubuntu-2004-shared/lib:\
/home/xinxin/work/depend_config/ncnn/usr/lib/x86_64-linux-gnu:\
/home/xinxin/work/depend_config/mnn/lib:\
/home/xinxin/work/mobilenetv2/02_cpp/install/lib:\
/usr/local/cuda-12.4/targets/x86_64-linux/lib:\
$LD_LIBRARY_PATH"

# TensorFlow Lite
install/bin/cpp_inference_test ../model/mobilenet_v2_1.0_224.tflite ../input/fish_224x224.jpeg ../model/labels.txt

# ONNX Runtime
install/bin/cpp_inference_test ../model/mobilenet_v2_1.0_224.onnx ../input/fish_224x224.jpeg ../model/labels.txt

# TensorRT FP32/FP16/INT8 (è‡ªåŠ¨è¯†åˆ«ç²¾åº¦)
install/bin/cpp_inference_test ../model/mobilenet_v2_1.0_224_fp32.trt ../input/fish_224x224.jpeg ../model/labels.txt
install/bin/cpp_inference_test ../model/mobilenet_v2_1.0_224_fp16.trt ../input/fish_224x224.jpeg ../model/labels.txt
install/bin/cpp_inference_test ../model/mobilenet_v2_1.0_224_int8.trt ../input/fish_224x224.jpeg ../model/labels.txt

# NCNN FP32/INT8 (è‡ªåŠ¨è¯†åˆ«ç²¾åº¦)
install/bin/cpp_inference_test ../model/mobilenet_v2_1.0_224.param ../input/fish_224x224.jpeg ../model/labels.txt
install/bin/cpp_inference_test ../model/mobilenet_v2_1.0_224-int8.param ../input/fish_224x224.jpeg ../model/labels.txt

# MNN FP32/INT8 (è‡ªåŠ¨è¯†åˆ«ç²¾åº¦)
install/bin/cpp_inference_test ../model/mobilenet_v2_1.0_224.mnn ../input/fish_224x224.jpeg ../model/labels.txt
install/bin/cpp_inference_test ../model/mobilenet_v2_1.0_224_int8.mnn ../input/fish_224x224.jpeg ../model/labels.txt
```

## ğŸ“ æ–‡ä»¶è¯´æ˜

```
mobilenetv2/
â”œâ”€â”€ 02_cpp/
â”‚   â”œâ”€â”€ include/                     # å¤´æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ inference_backend.hpp    # æ ¸å¿ƒæ¥å£ - å¯¹åº”Pythonçš„InferenceBackend
â”‚   â”‚   â”œâ”€â”€ tflite_backend.hpp       # TFLiteåç«¯
â”‚   â”‚   â”œâ”€â”€ onnx_backend.hpp         # ONNXåç«¯  
â”‚   â”‚   â”œâ”€â”€ tensorrt_backend.hpp     # TensorRTåç«¯
â”‚   â”‚   â”œâ”€â”€ ncnn_backend.hpp         # NCNNåç«¯
â”‚   â”‚   â””â”€â”€ mnn_backend.hpp          # MNNåç«¯
â”‚   â”œâ”€â”€ src/                         # æºæ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ inference_backend.cpp    # å·¥å‚ç±»å®ç°
â”‚   â”‚   â”œâ”€â”€ tflite_backend.cpp       # TFLiteå…·ä½“å®ç°
â”‚   â”‚   â”œâ”€â”€ onnx_backend.cpp         # ONNXå…·ä½“å®ç°
â”‚   â”‚   â”œâ”€â”€ tensorrt_backend.cpp     # TensorRTå…·ä½“å®ç°
â”‚   â”‚   â”œâ”€â”€ ncnn_backend.cpp         # NCNNå…·ä½“å®ç°
â”‚   â”‚   â””â”€â”€ mnn_backend.cpp          # MNNå…·ä½“å®ç°
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”œâ”€â”€ cpp_inference_test.cpp   # ç»Ÿä¸€å¤šåç«¯æµ‹è¯•ç¨‹åº
â”‚   â”‚   â””â”€â”€ CMakeLists.txt           # å¯æ‰§è¡Œæ–‡ä»¶æ„å»ºé…ç½®
â”‚   â”œâ”€â”€ install/                     # æœ¬åœ°å®‰è£…ç›®å½•
â”‚   â”‚   â”œâ”€â”€ include/                 # å¯¼å‡ºå¤´æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ lib/                     # åŠ¨æ€åº“
â”‚   â”‚   â””â”€â”€ bin/                     # å¯æ‰§è¡Œæ–‡ä»¶
â”‚   â”œâ”€â”€ build_so.sh                  # æ„å»ºåŠ¨æ€åº“è„šæœ¬
â”‚   â”œâ”€â”€ build_exe.sh                 # æ„å»ºå¯æ‰§è¡Œæ–‡ä»¶è„šæœ¬  
â”‚   â”œâ”€â”€ run_test.sh                  # æµ‹è¯•è¿è¡Œè„šæœ¬
â”‚   â””â”€â”€ README_new.md                # æœ¬æ–‡æ¡£
â””â”€â”€ model/                           # æ¨¡å‹æ–‡ä»¶ (ä¸Pythonç‰ˆæœ¬å…±äº«)
    â”œâ”€â”€ mobilenet_v2_1.0_224.tflite     # TensorFlow Liteæ¨¡å‹
    â”œâ”€â”€ mobilenet_v2_1.0_224.onnx       # ONNXæ¨¡å‹
    â”œâ”€â”€ mobilenet_v2_1.0_224_fp32.trt   # TensorRT FP32å¼•æ“
    â”œâ”€â”€ mobilenet_v2_1.0_224_fp16.trt   # TensorRT FP16å¼•æ“
    â”œâ”€â”€ mobilenet_v2_1.0_224_int8.trt   # TensorRT INT8å¼•æ“
    â”œâ”€â”€ mobilenet_v2_1.0_224.param      # NCNNç½‘ç»œç»“æ„æ–‡ä»¶ (FP32)
    â”œâ”€â”€ mobilenet_v2_1.0_224.bin        # NCNNæƒé‡å‚æ•°æ–‡ä»¶ (FP32)
    â”œâ”€â”€ mobilenet_v2_1.0_224-int8.param # NCNNç½‘ç»œç»“æ„æ–‡ä»¶ (INT8)
    â”œâ”€â”€ mobilenet_v2_1.0_224-int8.bin   # NCNNæƒé‡å‚æ•°æ–‡ä»¶ (INT8)
    â””â”€â”€ labels.txt                      # ImageNetåˆ†ç±»æ ‡ç­¾
```

## ğŸ”§ å¸¸è§é—®é¢˜

**Q: ç¼–è¯‘æ—¶æ‰¾ä¸åˆ°ä¾èµ–åº“ï¼Ÿ**
A: ç¡®ä¿æ‰€æœ‰ç¯å¢ƒå˜é‡æ­£ç¡®è®¾ç½®ï¼Œç‰¹åˆ«æ˜¯ `LD_LIBRARY_PATH`ã€‚ä½¿ç”¨ `ldd ./install/bin/cpp_inference_test` æ£€æŸ¥ä¾èµ–ã€‚

**Q: è¿è¡Œæ—¶åŠ¨æ€åº“åŠ è½½å¤±è´¥ï¼Ÿ**  
A: C++ç‰ˆæœ¬ä½¿ç”¨RPATHæœºåˆ¶ï¼Œå¯æ‰§è¡Œæ–‡ä»¶ä¼šè‡ªåŠ¨æ‰¾åˆ° `install/lib/` ä¸‹çš„åŠ¨æ€åº“ã€‚å¦‚æœå¤±è´¥ï¼Œæ£€æŸ¥ï¼š
- æ˜¯å¦æ­£ç¡®è¿è¡Œäº† `./build_so.sh` å’Œ `./build_exe.sh`
- `install/lib/libmobilenet_inference.so` æ˜¯å¦å­˜åœ¨

**Q: é¢„å¤„ç†å‚æ•°ä¸Pythonç‰ˆæœ¬ä¸€è‡´å—ï¼Ÿ**
A: **å®Œå…¨ä¸€è‡´**ï¼æ‰€æœ‰åç«¯éƒ½ä½¿ç”¨MobileNetV2å®˜æ–¹é¢„å¤„ç†å‚æ•°ï¼š`(pixel - 127.5) / 127.5`ï¼Œç¡®ä¿ä¸Pythonç‰ˆæœ¬æ•°å€¼ç²¾åº¦å®Œå…¨åŒ¹é…ã€‚

**Q: æ¡ä»¶ç¼–è¯‘å¦‚ä½•å·¥ä½œï¼Ÿ**
A: CMakeä¼šè‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„ä¾èµ–åº“ï¼š
- æ‰¾ä¸åˆ°TensorFlow Liteæ—¶è·³è¿‡TFLiteåç«¯
- æ‰¾ä¸åˆ°CUDAæ—¶è·³è¿‡TensorRTåç«¯  
- æ‰¾ä¸åˆ°NCNNæ—¶è·³è¿‡NCNNåç«¯
- è‡³å°‘éœ€è¦ä¸€ä¸ªåç«¯æ‰èƒ½ç¼–è¯‘æˆåŠŸ

## ğŸ¯ æ ¸å¿ƒä»·å€¼

**å®Œæ•´C++å®ç°**ï¼šä»Pythonåˆ°C++çš„ç«¯åˆ°ç«¯ç§»æ¤ï¼Œæ€§èƒ½å¤§å¹…æå‡

**æ¶æ„å®Œå…¨å¯¹é½**ï¼š1:1å¤ç°Pythonç‰ˆæœ¬çš„è®¾è®¡æ¨¡å¼ï¼Œä¾¿äºç»´æŠ¤

**åŠ¨æ€åº“æ¶æ„**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒç‹¬ç«‹éƒ¨ç½²å’Œé›†æˆ

**ç²¾åº¦å®Œç¾ä¸€è‡´**ï¼šç»Ÿä¸€é¢„å¤„ç†ç¡®ä¿æ•°å€¼ç²¾åº¦ä¸Pythonç‰ˆæœ¬å®Œå…¨åŒ¹é…

**æ€§èƒ½æ˜¾è‘—æå‡**ï¼š
- **CPUä¼˜åŒ–**ï¼šNCNNæ€§èƒ½æ¯”TFLiteå¿«7å€
- **GPUåŠ é€Ÿ**ï¼šTensorRTæ¯”CPUå¿«20å€ï¼Œæ¥è¿‘1000 FPS
- **é‡åŒ–åŠ é€Ÿ**ï¼šINT8ç‰ˆæœ¬å¤§å¹…å‡å°æ¨¡å‹ä½“ç§¯ï¼Œç²¾åº¦æŸå¤±è½»å¾®

**å·¥ç¨‹å®è·µå°±ç»ª**ï¼šæ¡ä»¶ç¼–è¯‘ã€RPATHä¼˜åŒ–ã€ç»Ÿä¸€æµ‹è¯•ç¨‹åºç­‰ç”Ÿäº§å°±ç»ªç‰¹æ€§
