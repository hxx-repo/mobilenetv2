#include "onnx_backend.hpp"
#include <iostream>
#include <chrono>
#include <algorithm>
#include <stdexcept>

namespace mobilenet_inference {

// æ„é€ å‡½æ•°ç°åœ¨æ˜¯å†…è”çš„ï¼Œè¿™é‡Œä¸éœ€è¦äº†

bool ONNXBackend::load_model() {
    try {
        // å®Œå…¨å¯¹é½Pythonçš„åŠ è½½é€»è¾‘
        std::cout << "åŠ è½½ONNXæ¨¡å‹: " << model_path_ << std::endl;
        std::cout << "ğŸ’» ä½¿ç”¨CPUä¼˜åŒ–æ‰§è¡Œ" << std::endl;
        
        // åˆ›å»ºenvä½œä¸ºæˆå‘˜å˜é‡ï¼Œç¡®ä¿åœ¨sessionç”Ÿå‘½å‘¨æœŸå†…å­˜åœ¨
        env_ = std::make_unique<Ort::Env>(ORT_LOGGING_LEVEL_WARNING, "ONNXBackend");
        Ort::SessionOptions session_options;
        
        // é…ç½®ä¼šè¯é€‰é¡¹ - å¯¹åº”Pythonçš„providers = ['CPUExecutionProvider']
        session_options.SetIntraOpNumThreads(1);
        session_options.SetInterOpNumThreads(1);
        session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);
        
        // åˆ›å»ºæ¨ç†ä¼šè¯ - å¯¹åº”Pythonçš„ort.InferenceSession(model_path, providers=providers)
        session_ = std::make_unique<Ort::Session>(*env_, model_path_.c_str(), session_options);
        
        // è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯ - å¯¹åº”Pythonçš„session.get_inputs()å’Œget_outputs()
        Ort::AllocatorWithDefaultOptions allocator;
        
        // è·å–æ¨¡å‹ä¿¡æ¯å¹¶è®¾ç½®åŸºç±»æˆå‘˜å˜é‡
        auto input_name_ptr = session_->GetInputNameAllocated(0, allocator);
        auto output_name_ptr = session_->GetOutputNameAllocated(0, allocator);
        
        input_name_ = std::string(input_name_ptr.get());
        output_name_ = std::string(output_name_ptr.get());
        
        // è·å–è¾“å…¥è¾“å‡ºå½¢çŠ¶å’Œç±»å‹ä¿¡æ¯
        auto input_info = session_->GetInputTypeInfo(0);
        auto output_info = session_->GetOutputTypeInfo(0);
        
        auto input_tensor_info = input_info.GetTensorTypeAndShapeInfo();
        auto output_tensor_info = output_info.GetTensorTypeAndShapeInfo();
        
        auto input_dims = input_tensor_info.GetShape();
        auto output_dims = output_tensor_info.GetShape();
        
        input_shape_.clear();
        output_shape_.clear();
        for (auto dim : input_dims) {
            input_shape_.push_back(static_cast<int>(dim));
        }
        for (auto dim : output_dims) {
            output_shape_.push_back(static_cast<int>(dim));
        }
        
        // è·å–æ•°æ®ç±»å‹
        auto input_element_type = input_tensor_info.GetElementType();
        input_dtype_ = (input_element_type == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) ? "float32" : "unknown";
        
        auto output_element_type = output_tensor_info.GetElementType();
        output_dtype_ = (output_element_type == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) ? "float32" : "unknown";
        
        // å®Œå…¨å¯¹é½Pythonçš„æ‰“å°æ ¼å¼
        std::cout << "âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ" << std::endl;
        std::cout << "   è¾“å…¥åç§°: " << input_name_ << std::endl;
        std::cout << "   è¾“å…¥å½¢çŠ¶: [";
        for (size_t i = 0; i < input_shape_.size(); ++i) {
            std::cout << input_shape_[i];
            if (i < input_shape_.size() - 1) std::cout << ", ";
        }
        std::cout << "]" << std::endl;
        std::cout << "   è¾“å…¥æ•°æ®ç±»å‹: " << input_dtype_ << std::endl;
        std::cout << "   è¾“å‡ºåç§°: " << output_name_ << std::endl;
        std::cout << "   è¾“å‡ºå½¢çŠ¶: [";
        for (size_t i = 0; i < output_shape_.size(); ++i) {
            std::cout << output_shape_[i];
            if (i < output_shape_.size() - 1) std::cout << ", ";
        }
        std::cout << "]" << std::endl;
        std::cout << "   è¾“å‡ºæ•°æ®ç±»å‹: " << output_dtype_ << std::endl;
        std::cout << "   æ‰§è¡Œæä¾›ç¨‹åº: [\"CPUExecutionProvider\"]" << std::endl;
        
        return true;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ ONNXæ¨¡å‹åŠ è½½å¤±è´¥: " << e.what() << std::endl;
        return false;
    }
}

std::vector<float> ONNXBackend::preprocess(const std::string& image_path) {
    // å®Œå…¨å¯¹é½Pythonçš„é¢„å¤„ç†é€»è¾‘
    cv::Mat img = cv::imread(image_path);
    if (img.empty()) {
        throw std::runtime_error("æ— æ³•è¯»å–å›¾åƒ: " + image_path);
    }
    
    // è½¬æ¢ä¸ºRGBæ ¼å¼ (OpenCVé»˜è®¤æ˜¯BGR)
    cv::Mat rgb_img;
    cv::cvtColor(img, rgb_img, cv::COLOR_BGR2RGB);
    
    // è°ƒæ•´å¤§å°ä¸º224x224 - å¯¹åº”Pythonçš„img.resize((224, 224))
    cv::Mat resized;
    cv::resize(rgb_img, resized, cv::Size(224, 224));
    
    // è½¬æ¢ä¸ºfloat32å¹¶å½’ä¸€åŒ– - å¯¹åº”Pythonçš„(img_array - 127.5) / 127.5
    resized.convertTo(resized, CV_32F);
    
    // MobileNetV2æ ‡å‡†åŒ–: (pixel - 127.5) / 127.5, å¯¹é½Pythonç‰ˆæœ¬
    resized = (resized - 127.5) / 127.5;
    
    // è½¬æ¢ä¸ºCHWæ ¼å¼ - å¯¹åº”Pythonçš„np.transpose(img_array, (2, 0, 1))
    std::vector<cv::Mat> channels(3);
    cv::split(resized, channels);
    
    std::vector<float> input_tensor;
    input_tensor.reserve(1 * 3 * 224 * 224);
    
    // æŒ‰CHWé¡ºåºæ’åˆ—ï¼šC(R,G,B) H W
    for (int c = 0; c < 3; ++c) {
        for (int h = 0; h < 224; ++h) {
            for (int w = 0; w < 224; ++w) {
                input_tensor.push_back(channels[c].at<float>(h, w));
            }
        }
    }
    
    return input_tensor;
}

std::vector<float> ONNXBackend::inference(const std::vector<float>& input_data) {
    try {
        // åˆ›å»ºè¾“å…¥å¼ é‡ - å¯¹åº”Pythonçš„session.run(None, {input_name: input_data})
        // ä½¿ç”¨åŸºç±»æˆå‘˜å˜é‡ input_shape_ è€Œä¸æ˜¯ç¡¬ç¼–ç 
        std::vector<int64_t> input_shape_int64;
        for (int dim : input_shape_) {
            input_shape_int64.push_back(static_cast<int64_t>(dim));
        }
        
        Ort::MemoryInfo memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
        
        // åˆ›å»ºè¾“å…¥å¼ é‡
        std::vector<Ort::Value> input_tensors;
        input_tensors.push_back(Ort::Value::CreateTensor<float>(
            memory_info, 
            const_cast<float*>(input_data.data()), 
            input_data.size(), 
            input_shape_int64.data(), 
            input_shape_int64.size()
        ));
        
        // æ‰§è¡Œæ¨ç† - ä½¿ç”¨åŸºç±»æˆå‘˜å˜é‡
        const char* input_names[] = {input_name_.c_str()};
        const char* output_names[] = {output_name_.c_str()};
        
        auto output_tensors = session_->Run(
            Ort::RunOptions{nullptr}, 
            input_names, 
            input_tensors.data(), 
            1, 
            output_names, 
            1
        );
        
        // è·å–è¾“å‡ºæ•°æ®
        float* output_data = output_tensors.front().GetTensorMutableData<float>();
        auto output_shape = output_tensors.front().GetTensorTypeAndShapeInfo().GetShape();
        
        size_t output_size = 1;
        for (auto dim : output_shape) {
            output_size *= dim;
        }
        
        std::vector<float> result(output_data, output_data + output_size);
        return result;
        
    } catch (const std::exception& e) {
        throw std::runtime_error("ONNXæ¨ç†å¤±è´¥: " + std::string(e.what()));
    }
}

std::vector<float> ONNXBackend::postprocess(const std::vector<float>& output_data) {
    // å¯¹é½Python: """ONNXåå¤„ç†ï¼šå‹ç¼©å¤šä½™ç»´åº¦"""
    // return np.squeeze(output_data)
    return output_data; // C++ä¸­vectorå·²ç»æ˜¯1ç»´çš„ï¼Œç›¸å½“äºsqueezeåçš„ç»“æœ
}

void ONNXBackend::cleanup() {
    // ONNXæ¸…ç†èµ„æºï¼šé‡Šæ”¾ä¼šè¯å’Œç¯å¢ƒ
    if (session_) {
        session_.reset();
    }
    if (env_) {
        env_.reset();
    }
    std::cout << "ONNXèµ„æºå·²æ¸…ç†" << std::endl;
}

} // namespace mobilenet_inference